 Το επόμενο κοινό είναι προσδοκημένο υπό δίκτυο Creative Commons. Η υποστηρική σας θα βοηθήσει να συνεχίσει να προσφέρει υψηλές ειδικές παιδικές ειδικές πράγματα για ελεύθερα. Για να κάνετε μια δονάση ή να παρακολουθείτε περισσότερα υλικά από χιλιάδες μαθητές MIT, επισκεφθείτε το MIT OpenCourseWare στηn ocw.mit.edu. Θα συνεχίσουμε σήμερα με την συζήτηση μας για κλασσική στατιστική. Θα ξεκινήσουμε με μια γρήγορη συζητησία του τι συζητήσαμε τελικά και μετά θα μιλήσουμε για δύο θέματα που κλειδίνουν πολλές από τις στατιστικές που συμβαίνουν στον πραγματικό κόσμο. Έτσι, δύο βασικές μέθοδες. Ένα είναι το μέθοδο της λινείας μετατραπής και το άλλο είναι βασικές μέθοδες και εργαλείες για να κάνουμε υποστηρικτικές δοκιμές. Οπότε, αυτοί δύο είναι θέματα που κάθε σχολητικά λιτέρα πρέπει να ξέρει κάτι για. Θα παρουσιάσουμε τις βασικές ιδέες και κοντέξεις συμβαίνοντας. Στην κλασσική στατιστική, βασικά έχουμε μια οικογένεια δυνατών μοντέλων για τον κόσμο. Ο κόσμος είναι μια δίκαιη διόρθωση που παρατηρούμε και έχουμε ένα μοντέλο για αυτό, αλλά όχι μόνο ένα μοντέλο, αλλά και μερικές κανδυνάτες μοντέλες. Κάθε μοντέλο κανδυνάτων αντιμετωπίζεται σε διαφορετική αξία για ένα παράμετρο θ, που δεν γνωρίζουμε. Στην αντίσταση με τις βασικές ιδέες, αυτό το θ αποδεικνύει να είναι μια συστατική που δεν γνωρίζουμε. Δεν είναι μοντέλωστη ως διευθυντική αξία, δεν υπάρχουν προφανείς που σχεδόν έχουν με θ. Υπάρχουν μόνο προφανείς για τις εξ. Σε αυτό το σύστημα, τι είναι μια αξιακή τρόπο να επιλέξουμε ένα αξίο για το παράμετρο θ? Μία γενική προσέγγιση είναι η προσέγγιση της αξιότητας, η οποία επιλέξει το θ, για το οποίο αυτή η ποσότητα είναι η μεγαλύτερη. Τι σημαίνει αυτό αξιότιμως? Προσπαθώ να βρω ένα αξίο του θ, υπό το οποίο τα δεδομένα που εξετάσαμε είναι οι πιο πιθανόμενες να έχουν συμβεί. Η σκέψη είναι οτι πρέπει να επιλέξω μεταξύ δύο επιλογών θ. Υπό το θ, το αξίο που εξετάσαμε θα είναι πολύ αδύνατο. Υπό το θ, το αξίο που εξετάσαμε θα έχει μια καλή πιθανότητα να συμβεί. Λοιπόν, επιλέξω το τελευταίο ως το αξίο του θ. Είναι ενδιαφέρουσα να κάνουμε την παραπάνωση με την προσέγγιση του Bayesian, την οποία συζητήσαμε την τελευταία φορά. Στην προσέγγιση του Bayesian, επίσης, εξετάζουμε πάνω από το θ, αλλά εξετάζουμε μια ποσότητα στην οποία η σχέση μεταξύ εξ και θ λειτουργεί την αντίθετη τρόπο. Εδώ, στον Bayesian κόσμο, το θ είναι μια αρκετή διαφορά, οπότε έχει μια διαδίκτυα. Μετά από την παρακολουθία του δαίτα, έχει μια αξιοπροσέγγιση. Και βρίσκουμε το αξίο του θ, που είναι πιθανότητα υπό την αξιοπροσέγγιση. Όπως συζητήσαμε την τελευταία φορά, όταν κάνεις αυτήν την εξετάσεις, η αξιοπροσέγγιση δημιουργείται από αυτήν την εξετάσεις, ο ορισμός δεν αφορά. Και αν πήγαμε να πάρουμε μια πριωρή, η οποία είναι πλατή, η οποία είναι μια συστατική, ανεξαρτητή από το θ, τότε αυτό το τρόπο θα πηγαίνει. Και, συντακτικά, τουλάχιστον, τα δύο προσέγγισης δουν τα ίδια. Λοιπόν, συντακτικά, ή πραγματικά, η αξιοπροσέγγιση του πλήρους είναι η ίδια σαν την Bayesian αξιοπροσέγγιση, με την οποία αξιολογείς μια πριωρή, η οποία είναι πλατή, ώστε όλες τις δυνατές αξιόπισεις του θ είναι αξιωματικά δυνατές. Φιλοσοφικά, όμως, είναι πολύ διαφορετικά πράγματα. Εδώ, βάζω την πιο δυνατή αξιοπροσέγγιση του θ. Εδώ, βάζω την αξιοπροσέγγιση του θ, υπό την οποία η δεδομένη δεδομένη θα ήταν πιο δυνατή να συμβαίνει. Η αξιοπροσέγγιση του θ είναι μια μέθοδος για κοινό στόχο, και λοιπόν, είναι εφαρμογμένη σε πολλές διαφορετικές εξοπλισμές. Υπάρχει ένα ειδικό είδος εξοπλισμού, με το οποίο μπορείς να ξεχάσεις την αξιοπροσέγγιση του θ και να έρθεις με ένα αξιόπισμα με μια λεπτομένη τρόπο. Και αυτό είναι το περίπτωμα που προσπαθείς να αξιολογήσεις το μήνυμα της διστριβιζόμενης του X, όπου X είναι μια λεπτομένη διαφορετική. Βλέπεις πολλά ανεξαρτημένα, ιδανικά διστριβιζόμενα λεπτομένες διαφορετικές, X1 μέχρι Xn, όλες έχουν την ίδια διστριβιζόμενη ως αυτή η X, οπότε έχουν μια κοινή μήνυμα. Δεν γνωρίζουμε το μήνυμα που θέλουμε να αξιολογήσουμε. Τι είναι πιο φυσικό, αν δεν πάμε από την αξία των αξίων που έχουμε παρατηρήσει. Λοιπόν, γίνεται πολλά X, πάμε από την αξία των αξίων, και εξετάζουμε ότι αυτό θα είναι ένα αξιόπισμα αρκετός του αληθινού μήνυμα του διστριβιζόμενου. Και, βέβαια, ξέρουμε από το σκληρό λόγο μεγάλων αριθμών ότι αυτό το αξιόπισμα συμμετέχει, σε πιθανότητα, στο αληθινό μήνυμα του διστριβιζόμενου. Το άλλο που είπαμε την τελευταία φορά είναι ότι, εκτός από να δώσουμε ένα αριθμό, μπορούμε να θέλουμε να δώσουμε έναν αριθμό που μας πει κάτι για το πού μπορούμε να πιστεύουμε ότι θηλώνει η θηλυκία. Και το 1-α αριθμό της εμπιστοσύνης είναι ένα αριθμό που γεννηθεί βασίως από τα δεδομένα. Λοιπόν, είναι ένα αριθμό από αυτό το αριθμό, σε αυτό το αριθμό. Αυτά τα αριθμά γράφονται με κεφαλαίες, γιατί είναι αριθμότερα, γιατί ανάγκες στις δεδομένες που έχουμε δει. Και αυτό μας δίνει έναν αριθμό, και θέλουμε αυτό το αριθμό να έχει την πραγματικότητα ότι η θηλυκία είναι μέσα από αυτό το αριθμό με υψηλή πιθανότητα. Λοιπόν, τυπικά θα πάμε να πάμε από το 1-α αριθμό να είναι ένα αριθμό όπως το 95%, για παράδειγμα, σε ο οποίο πράγμα έχουμε ένα 95% αριθμό της εμπιστοσύνης. Όπως συζητήσαμε την τελευταία φορά, είναι σημαντικό να έχουμε την σωστή εντόπιση για το 95%. Τι δεν σημαίνει είναι το επόμενο, ο αριθμός που δεν γνωρίζεται έχει 95% πιθανότητα να είναι στο αριθμό που έχουμε δημιουργηθεί. Αυτό είναι γιατί ο αριθμός που δεν γνωρίζεται δεν είναι μια αρχαία αριθμό. Είναι ένα συστηματικό. Όταν δημιουργούμε το αριθμό, είτε είναι μέσα είτε είναι δεξιά, αλλά δεν υπάρχουν πιθανότητες συμμετείχνου. Αλλά οι πιθανότητες πρέπει να εμφανίζονται πάνω από το εξωτερικό αριθμό. Αυτό που λέει ένα σύστημα σαν αυτό είναι ότι αν έχω ένα συστηματικό για να γεννηθώ 95% πιθανότητα αριθμού, τότε, όταν χρησιμοποιώ αυτό το συστηματικό, θα έχω ένα εξωτερικό αριθμό και θα έχει 95% πιθανότητα να καταφέρει την αληθινή αριθμό της θητας. Οπότε, την περισσότερη φορά που χρησιμοποιώ ένα συστηματικό για να γεννηθώ πιθανότητες αριθμού, η αληθινή θητα θα συμμετείχει μέσα από αυτή την αριθμό με πιθανότητα 95%. Λοιπόν, η πιθανότητα σε αυτό το συστημα είναι με σχέση με την αριθμότητα της πιθανότητας, δεν είναι με σχέση με την θητα, γιατί η θητα δεν είναι πιθανότητα. Πώς κατασκευάζεται ένα πιθανότητα αριθμού? Υπάρχουν διάφορες τρόπες να το συζητήσουμε, αλλά, όταν συζητήσουμε την αριθμότητα της θητας για ένας πιθανότητας, το κάνουμε απλά χρησιμοποιώντας το θεωρίαμπρο-σύστημα. Βασικά, πάρουμε την αριθμότητα της θητα, η θητα της παραδείγματος, και πάρουμε ένα συστηματικό αριθμό στην δεξιά και στην πράττα της θητας παραδείγματος, και επιλέγουμε την μεγάλη αριθμό αυτής με την παραδείγματος των κανονικών τραπεζών. Αν η αριθμότητα 1-α είναι 95%, θα παρακολουθούμε το 97,5% της κανονικής διαδικασίας, θα βρειτε το συστηματικό αριθμό που αντιμετωπίζει αυτή την αριθμό από τις κανονικές τραπεζές, και θα κατασκευάσουμε το αριθμό της εμπιστοσύνης με αυτή την ομολογία. Αυτό σας δίνει μια αρκετή μηχανική τρόπη για τη διαδικασία των αριθμών της εμπιστοσύνης όταν αριστεράτε το διεθνόμενο. Αυτή την κανόντα διαδικάζεται μια συμπεριφορά. Η συμπεριφορά είναι το θεωριόμενο κεντρικό κεφάλαιο. Αποδεχτούμε ότι το διεθνόμενο κεφάλαιο είναι μια τρομερά δυνατή αριθμότητα, η οποία είναι πιο ή λεπτομέρως σωστά όταν η N είναι μεγάλη. Αυτό μας λέει το θεωρία κεντρικού κεφάλαιου. Και μερικές φορές μπορούμε να κάνουμε κάποια προσοχή για προσοχή, γιατί συχνά δεν γνωρίζουμε το αλήθειαστο αριθμό του σιγμά. Πρέπει λοιπόν να κάνουμε κάποια δουλειά, για να καταγραφούμε το σιγμά από τα δεδάτα. Το σιγμά είναι φυσικά η στάνταρδικη αριθμότητα των εξελίξεων. Μπορούμε να το καταγραφούμε από τα δεδάτα ή μπορούμε να έχουμε έναν υψηλό σιγμά και να το χρησιμοποιούμε. Ας προχωρήσουμε στη νέα θέματα. Πολύ πολλές στατιστικές στον πραγματικό κόσμο είναι από τη συνεχή πλήρωση. Από τη συνεχή πλήρωση. Αποφασίστε ότι το x είναι το σημείο SAT του σπουδαίου στρατιωτή και το y είναι το GPA MIT του ίδιου σπουδαίου στρατιωτή. Από τη συνεχή πλήρωση. Προσέξτε ότι υπάρχει μια σύνδεση μεταξύ αυτών. Παρακολουθείτε τα δεδάτα για διαφορετικούς σπουδαίους και το σχηματίζετε για έναν τυπικό σπουδαίο. Αυτό θα είναι το σημείο SAT και αυτό θα είναι το GPA MIT. Και πλούτσετε όλα αυτά τα δεδάτα σε ένα διαγράμμα XY. Μπορείτε να πιστεύετε, είναι αρκετό να πιστεύετε, ότι υπάρχει συστηματική σύνδεση οπότε, οι άνθρωποι που είχαν μεγαλύτερες σχολεία SAT μπορούσαν να έχουν μεγαλύτερη GPA στο σχολείο. Αυτό μπορεί ή δεν μπορεί να είναι αλήθεια. Θέλετε να κατασκευάσετε ένα μοντέλο τέτοιου και να δείτε σε ποιο σημείο η σύνδεση αυτής του τύπου είναι αλήθεια. Άρα μπορείτε να υποθετήσετε ότι το πραγματικό κόσμο είναι οδηγημένο από ένα μοντέλο τέτοιου. Αυτό είναι μια αλήθεια σύνδεση μεταξύ του σημείου SAT και του σχολείου GPA. Είναι μια αλήθεια σύνδεση με κάποιες παραμήτες θ0 και θ1 που δεν γνωρίζουμε. Αποδεχτούμε μια αλήθεια σύνδεση για τα δεδομένα και, εξαρτάτα από τις επιλογές θ0 και θ1, μπορεί να είναι μια διαφορετική στρατιώδη για αυτά τα δεδομένα. Αυτό που θέλουμε να βρούμε είναι ένα μοντέλο τέτοιου για να εξηγήσουμε τα δεδομένα. Βέβαια, θα υπάρχει κάποια αρνητικότητα, οπότε, γενικά, θα είναι απασχολητικό να βρούμε μια στρατιώδη που θα περάσει σε όλες τις δεδομένα. Ας προσπαθήσουμε να βρούμε την καλύτερη στρατιώδη που καλύτερα αναφερθεί στις δεδομένες. Και εδώ είναι πώς το κάνουμε. Αντιμετωπίζουμε κάποιες συγκεκριμένες αριθμοί θ0 και θ1, θα μας δίνουν μια στρατιώδη. Αντίθετα αυτή, μπορούμε να κάνουμε προδοκίες. Για ένα στουδεντ που είχε αυτό το x, το μοντέλο που έχουμε θα προδοκίσει ότι το y θα είναι αυτό το αριθμό. Το πραγματικό y είναι κάτι άλλο, και οπότε αυτή η ποσότητα είναι η αρρώστια που θα κάνει το μοντέλο μας στο προδοκίματο του y αυτού του στουδεντή. Θα θέλαμε να επιλέξουμε μια στρατιώδη με την οποία οι προδοκίματες είναι όσο καλές όσο μπορούν. Τι εννοούμε με όσο καλές όσο μπορούν? Σε αυτό το κριτήριο θα πάμε να δούμε την ερροή προδοκίματος που κάνει ο μοντέλος για κάθε στουδεντή, θα πάμε να πάμε και να το προσθέσουμε σε όλες τις ποσότητες μας. Αυτό που παρακολουθούμε είναι η αριθμό αυτής της ποσότητας με αυτήν, αυτήν, και τέτοια. Θα προσθέσουμε όλες αυτές τις ποσότητες και θα ήθελα να βρούμε μια στρατιώδη για την οποία η αριθμό αυτής της προδοκίματος είναι όσο μικρή όσο μπορεί. Αυτή είναι η διαδικασία. Έχουμε τα δεξιά μας, τα εξ και τα ευ. Και θα βρούμε θ, το καλύτερο μοντέλο αυτής της τύπως, το καλύτερο δυνατό μοντέλο με την ερροή προδοκίματος. Αυτή είναι μια μέθοδος που μπορεί να βάλεις από το κουμπί και να πεις αυτό είναι πώς θα κατασκευάσω το μοντέλο μου και ακριβώς αρκετά αρκετά λογικό. Και ακριβώς ακριβώς αρκετά λογικό ακόμα και αν δεν ξέρεις τίποτα για την προσοχή. Αλλά έχει κάποια προσοχητική κατασκευή. Ξέρει ότι ναι, μπορείς να προστατεύσεις αυτήν την μέθοδο με προσοχητικές προστασίες υπό κάποιες υποσχέσεις. Ας κάνουμε μια προσοχητική κατασκευή που θα μας διευθυντήσει σε αυτήν την συγκεκριμένη τρόπο της προσοχής των παραμετωρικών. Εδώ είναι μια προσοχητική κατασκευή. Προσέξω έναν στουδεντ who had a specific SAT score and that could be done at random but also could be done in a systematic way. That is, I pick a student who had an SAT of 600, a student of 610 all the way to 1400 or 1600 whatever the right number is. I pick all those students and I assume that for a student of this kind there is a true model that tells me that their GPA is going to be a random variable which is something predicted by their SAT score plus some randomness, some random noise. And I model that random noise by independent normal random variables with zero mean and a certain variance. So this is a specific probabilistic model and now I can think about doing maximum likelihood estimation for this particular model. So to do maximum likelihood estimation here I need to write down the likelihood of the y's that I have observed. What's the likelihood of the y's that I have observed? Well a particular w has a likelihood of the form e to the minus w squared over 2 sigma squared. That's the likelihood of a particular w. The probability or the likelihood of observing a particular value of y that's the same as the likelihood that w takes a value of y minus this minus that. So the likelihood of the y's is of this form. Think of this as just being the wi squared. So this is the dense and if we have multiple data you multiply the likelihood of the different y's. So you have to write something like this. Since the w's are independent that means that the y's are also independent. The likelihood of a y vector is the product of the likelihood of the individual y's. The likelihood of every individual y is of this form where w is yi minus these two quantities. So this is the form that the likelihood function is going to take under this particular model and under the maximum likelihood methodology we want to maximize this quantity with respect to theta naught and theta 1. Now to do this maximization you might as well consider the logarithm and maximize the logarithm which is just the exponent up here. Maximizing this exponent because we have a minus sign is the same as minimizing the exponent without the minus sign. Sigma squared is a constant so what you end up doing is minimizing this quantity here which is the same as what we had in our linear regression methods. So conclusion, you might choose to do linear regression in this particular way just because it looks reasonable or plausible or you might interpret what you're doing as maximum likelihood estimation in which you assume a model of this kind where the noise terms are normal random variables with the same distribution independent identically distributed. So linear regression implicitly makes an assumption of this kind. It's doing maximum likelihood estimation as if the world was really described by a model of this form and with the W's being random variables. So this gives us at least some justification that this particular approach to fitting lines to data is not so arbitrary but it has a sound footing. Okay so then once you accept this formulation as being a reasonable one, what's the next step? The next step is to see how to carry out this minimization. This is not a very difficult minimization to do. The way it's done is by setting the derivatives of this expression to 0. Now because this is a quadratic function of theta 0 and theta 1, when you take the derivatives with respect to theta 0 and theta 1, you get linear functions of theta 0 and theta 1 and you end up solving a system of linear equations in theta 0 and theta 1 and it turns out that there's a very nice and simple formulas for the optimal estimates of the parameters in terms of the data. And the formulas are these ones. I said that these are nice and simple formulas. Let's see why. How can we interpret them? So suppose that the suppose that the world is described by a model of this kind where the X's and Y's are random variables and where W is a noise term that's independent of X. We're assuming that the linear model is indeed true but not exactly true. There's always some noise associated with any particular data point that we obtain. So if a model of this kind is true and the W's have 0 mean, then we have that the expected value of Y would be theta 0 plus theta 1 expected value of X and because W has 0 mean, there's no extra term. So in particular, theta 0 would be equal to expected value of Y minus theta 1 expected value of X. So let's use this equation to try to come up with a reasonable estimate of theta 0. I do not know the expected value of Y but I can estimate it. How do I estimate it? I look at the average of all the Y's that I have obtained. So I replace this, I estimate it with the average of the data I have seen. Here, similarly with the X's. I might not know the expected value of X's but I have data points for the X's. I look at the average of all my data points. I come up with an estimate of this expectation. Now, I don't know what theta 1 is but my procedure is going to generate an estimate of theta 1, call it theta 1 hat, and once I have this estimate, then a reasonable person would estimate theta 0 in this particular way. So that's how my estimate of theta 0 is going to be constructed. It's this formula here. We have not yet addressed the harder question, which is how to estimate theta 1 in the first place. So to estimate theta 0, I assume that I already had an estimate for theta 1. OK. The right formula for the estimate of theta 1 happens to be this one. It looks messy but let's try to interpret it. What I'm going to do is I'm going to take this model for simplicity, let's assume that the random variables have 0 means and see how we might estimate how we might try to estimate theta 1. Let's multiply both sides of this equation by x. So we get y times x equals theta 0 plus theta 0 x plus theta 1 x squared plus x w. And now take expectations of both sides. If I have 0 mean random variables, the expected value of y x is just the covariance of x with y. I have assumed that my random variables have 0 means, so the expectation of this is 0. This one is going to be the variance of x, so I have theta 1 variance of x and since I'm assuming that my random variables have 0 mean and I'm also assuming that w is independent of x, this last term also has 0 mean. So, under such a probabilistic model, this equation is true. If we knew the variance and the covariance, then we would know the value of theta 1. But we only have data, we do not necessarily know the variance and the covariance, but we can estimate it. What's a reasonable estimate of the variance? A reasonable estimate of the variance is this quantity here divided by n, and the reasonable estimate of the covariance is that numerator divided by n. So, this is my estimate of the mean. I'm looking at the squared distances from the mean, and I average them over lots and lots of data. This is a most reasonable way of estimating the variance of our distribution. And similarly, here this quantity, the expected value of this quantity is the covariance of x with y, and if we have lots and lots of data points, this quantity here is going to be a very good estimate of the covariance. So, basically, the way what this formula does is, one way of thinking about it, is that it starts from this relation, which is true, exactly, but estimates the covariance and the variance on the basis of the data, and then uses these estimates to come up with an estimate of theta one. So, this gives us a probabilistic interpretation of the formulas that we have for the way that the estimates are constructed. If you are willing to assume that this is the true model of the world, the structure of the true model of the world, except that you do not know means and covariances, and variances, then this is a natural way of estimating those unknown parameters. All right, so we have a closed-form formula. We can apply it whenever we have data. Now, linear regression is a subject on which there are whole courses and whole books that are given, and the reason for that is that there's a lot more that you can bring into the topic, and many ways that you can elaborate on the simple solution that we got for the case of two parameters and only two random variables. So, let me give you a little bit of flavor of what are the topics that come up when you start looking into linear regression in more depth. So, in our previous, in our discussion so far, we made a linear model in which we're trying to explain the values of one variable in terms of the values of another variable. We're trying to explain GPAs in terms of SAT scores, or we're trying to predict GPAs in terms of SAT scores. But maybe your GPA is affected by several factors. For example, maybe your GPA is affected by your SAT score, also the income of your family, the years of education of your grandmother, and many other factors like that. So, you might write down a model in which I believe that GPA has a relation, which is a linear function of all these other variables that I mentioned. So, perhaps you have a theory of what determines performance at college, and you want to build a model of that type. How do we go about in this case? Well, again, we collect data points. We look at the I-th student who has a college GPA. We record their SAT score, their family income, and grandmother's years of education. So, this is one data point that is for one particular student. We postulate a model of this form. For the I-th student, this would be the mistake that our model makes if we have chosen specific values for those parameters, and then we go and choose the parameters that are going to give us again the smallest possible sum of squared errors. So, philosophically, it's exactly the same as what we were discussing before, except that now we're including multiple explanatory variables in our model, instead of a single explanatory variable. So, that's the formulation. What do you do next? Well, to do this minimization, you're going to take derivatives with, once you have your data, you have a function of these three parameters. You take the derivative with respect to the parameters, set the derivative equal to zero, you get a system of linear equations, you throw that system of linear equations to the computer, and you get numerical values for the optimal parameters. There are no nice closed-form formulas of the type that we had in the previous slide, when you're dealing with multiple variables, unless you're willing to go into metrics notation. In that case, you can again write down closed-form formulas, but they will be a little less intuitive than what we had before. But the moral of the story is that numerically, this is a procedure that's very easy. It's a problem, an optimization problem that the computer can solve for you, and it can solve it for you very quickly, because all that it involves is solving a system of linear equations. Now, when you choose your explanatory variables, you may have some choices. One person may think that your GPA has something to do with your SAT score. Some other person may think that your GPA has something to do with the square of your SAT score. And that other person may want to try to build a model of this kind. When would you want to do this? Suppose that the data that you have look like this. If the data look like this, then you might be tempted to say, well, a linear model does not look right, but maybe a quadratic model will give me a better fit for the data. If you want to fit a quadratic model to the data, then what you do is you take x squared as your explanatory variable instead of x, and you build a model of this kind. There's nothing really different in models of this kind compared to models of that kind. They are still linear models because we have thetas showing up in a linear fashion. What you take as your explanatory variables, whether it's x, whether it's x squared, or whether it's some other function that you chose, some general function h of x, doesn't make a difference. So think of your h of x as being your new x. So you can formulate the problem exactly the same way, except that instead of using x's, you choose h of x's. So it's basically a question, do I want to build a model that explains y's based on the values of x, or do I want to build a model that explains y's on the basis of the values of h of x, which is the right value to use? And we, with this picture here, we see that it can make a difference. A linear model in x might be a poor fit, but a quadratic model might give us a better fit. So this brings to the topic of how to choose your functions, h of x, if you're dealing with a real world problem. So in a real world problem, you're just given x's and y's, and you have the freedom of building models of any kind you want. You have the freedom of choosing a function h of x of any type that you want. So this turns out to be a quite difficult and tricky topic, because you may be tempted to overdo it. For example, I got my ten data points, and I could say, okay, I'm going to choose an h of x, I'm going to choose h of x and actually multiple h's of x to do a multiple linear regression, in which I'm going to build a model that uses a tenth degree polynomial. If I choose to fit my data with a tenth degree polynomial, I'm going to fit my data perfectly, but I may obtain a model that does something like this, and goes through all my data points. So I can make my prediction errors extremely small if I use lots of parameters, and if I choose my h functions appropriately. But clearly, this would be garbage. If you get those data points, and you say, here's my model that explains them, that has a polynomial that keeps going up and down, then you're probably doing something wrong. So choosing how complicated those functions, the h's, should be, and how many explanatory variables to use, is a very delicate and deep topic, in which there's deep theory that tells you what you should do, and what you shouldn't do. But the main thing that one should avoid doing, is having too many parameters in your model, when you have too few data. So if you only have ten data points, you shouldn't have ten free parameters. With ten free parameters, you will be able to fit your data perfectly, but you wouldn't be able to really rely on the results that you're seeing. Okay. Now, in practice, when people run linear regressions, they do not just give point estimates for the parameters theta, but similar to what we did for the case of estimating the mean of a random variable, you might want to give confidence intervals. That sort of tells you how much randomness there is, when you estimate each one of the particular parameters. There are formulas for building confidence intervals, for the estimates of the thetas. We're not going to look at them, it would take too much time. Also, you might want to estimate the variance in the noise that you have in your model. That is, if you're pretending that your true model is of the kind we were discussing before, namely y equals theta 2 plus theta 1 x plus w, and w has a variance sigma squared, you might want to estimate this, because it tells you something about the model, and this is called standard error. It puts a limit on how good predictions your model can make. Even if you have the correct theta 0 and theta 1, and somebody tells you x, you can make a prediction about y, but that prediction will not be accurate, because there is this additional randomness, and if that additional randomness is big, then your predictions will also have a substantial error in them. There's another quantity that gets reported usually. This is part of the computer output that you get when you use a statistical package, which is called R squared, and it's a measure of the explanatory power of the model that you have built using linear regression. Using linear regression. Instead of defining R squared exactly, let me give you a sort of analogous quantity that's involved. After you do your linear regression, you can look at the following quantity. You look at the variance of y, which is something that you can estimate from data. This is how much randomness there is in y, and compare it with the randomness that you have in y, but conditioned on x. This quantity tells me if I knew x, how much randomness would there still be in my y? If I know x, I have more information, so y is more constrained. There's less randomness in y. This is the randomness in y if I don't know anything about x. So naturally, this quantity would be less than 1. And if this quantity is small, it would mean that whenever I know x, then y is very well known, which essentially tells me that knowing x allows me to make very good predictions about y. Knowing x means that I'm explaining away most of the randomness in y. So if you read a statistical study that uses linear regression, you might encounter statements of the form 60% of students' GPA is explained by the family income. If you read the statements of this kind, it really refers to quantities of this kind. Out of the total variance in y, how much variance is left after we build our model? So if only 40% of the variance of y is left after we build our model, that means that x explains 60% of the variation in y. So the idea is that randomness in y is caused by multiple sources, our explanatory variable and random noise, and we ask the question, what percentage of the total randomness in y is explained by variations in the x parameter, and how much of the total randomness in y is attributed just to random effects? So if you have a model that explains most of the variation in y, then you can think that you have a good model that tells you something useful about the model. Now there's lots of things that can go wrong when you use linear regression, and there's many pitfalls. One pitfall happens when you have this situation that's called heteroscedasticity. So suppose your data are of this kind. So what's happening here? You seem to have a linear model, but when x is small, you have a very good model, so this means that w has small variance when x is here. On the other hand, when x is there, you have a lot of randomness. This would be a situation in which the w's are not identically distributed, but the variance of the w's, of the noise, has something to do with the x's. So if we have different regions of our x space, we have different amounts of noise. What will go wrong in this situation? Since we're trying to minimize sum of squared errors, we're really paying attention to the biggest errors, which will mean that we are going to pay attention to these data points, because that's where the big errors are going to be. So the linear regression formulas will end up building a model based on these data, which are the most noisy ones, instead of those data that are nicely stacked in order. Clearly, that's not the right thing to do, so you need to change something and use the fact that the variance of w changes with the x's, and there are ways of dealing with it, and it's something that one needs to be careful about. Another possibility of getting into trouble is if you're using multiple explanatory variables that are very closely related to each other. So, for example, suppose that I try to predict your GPA by looking at your SAT the first time that you took it, plus your SAT the second time that you took your SATs. I'm assuming that almost everyone takes the SAT more than once. So, suppose that you had a model of this kind. Well, SAT in your first try and SAT in your second try are very likely to be fairly close. And you could think of coming up with estimates in which this is ignored, and you build a model based on this, or an alternative model in which this term is ignored, and you make predictions based on the second SAT. And both models are likely to be, essentially, as good as the other one, because these two quantities are essentially the same. So, in that case, your thetas that you estimate are going to be very sensitive in little details of the data. You change your data, you have your data, and your data tell you that this coefficient is big and that coefficient is small. You change your data just a tiny bit, and your thetas would drastically change. So this is a case in which you have multiple explanatory variables, but they're redundant in the sense that they're very closely related to each other, and perhaps with a linear relation. So one must be careful about the situation, and do special tests to make sure that this doesn't happen. Finally, the biggest and most common blunder is that you run your linear regression, you get your linear model, and then you say, oh, okay, y is caused by x according to this particular formula. Well, all that we did was to identify a linear relation between x and y. This doesn't tell us anything whether it's y that causes x, or whether it's x that causes y, or maybe both x and y are caused by some other variable that we didn't think about. So, building a good linear model that has small errors does not tell us anything about causal relations between the two variables. It only tells us that there's a close association between the two variables. If you know one, you can make predictions about the other, but it doesn't tell you anything about the underlying physics, that there's some physical mechanism that introduces the relation between those variables. Okay, that's it about linear regression. Let us start the next topic, which is hypothesis testing, and we're going to continue with it next time. Okay. So, here, instead of trying to estimate continuous parameters, we have two alternative hypotheses about the distribution of the x random variable. So, for example, for example, a random variable could be either distributed according to this distribution under H0, or it might be distributed according to this distribution under H1. And we want to make a decision which distribution is the correct one. So, we're given those two distributions and some common terminology is that one of them is the null hypothesis, sort of the default hypothesis, and we have some alternative hypothesis, and we want to check whether this one is true or that one is true. So, you obtain a data point and you want to make a decision. In this picture, what would a reasonable person do to make a decision? They would probably choose a certain threshold, Xi, and decide that H1 is true if your data falls in this interval, and decide that H0 is true if you fall on this side. So, that would be a reasonable way of approaching the problem. More generally, you take the set of all possible x's, and you divide the set of possible x's into two regions. One is the rejection region, in which you decide H1, or you reject H0, and the complement of that region is where you decide H0. So, this is the x space of your data. In this example here, x was one-dimensional, but in general, x is going to be a vector. All the possible data vectors that you can get, they're divided into two types. If it falls in this set, you make one decision. If it falls in that set, you make the other decision. So, how would you characterize the performance of a particular way of making a decision? Suppose I chose my threshold. I may make mistakes of two possible types. Perhaps H0 is true, but my data happens to fall here, in which case I make a mistake, and this would be a false rejection of H0. If my data falls here, I reject H0, I decide H1, whereas H0 was true. The probability of this happening, let's call it alpha, but there is another kind of error that can be made. Suppose that H1 was true, but by accident, my data happens to fall on that side, then I'm going to make an error again. I'm going to decide H0, even though H1 was true. How likely is this to occur? This would be the area under this curve here. And that's the other type of error that can be made, and beta is the probability of this particular type of error. Both of these are errors. Alpha is the probability of error of one kind, beta the probability of an error of the other kind. You would like the probabilities of error to be small, so you would like to make both alpha and beta as small as possible. Unfortunately, that's not possible. There's a trade-off. If I move my threshold this way, then alpha becomes smaller, but beta becomes bigger. So there's a trade-off. If I make my rejection region smaller, one kind of error is less likely, but the other kind of error becomes more likely. So we got this trade-off. So what do we do about it? How do we move systematically? How do we come up with rejection regions? What the theory basically tells you is it tells you how you should create those regions, but it doesn't tell you exactly how. It tells you the general shape of those regions. For example, in this here, the theory that tells us that the right thing to do would be to put a threshold and make decisions one way to the right, one way to the left, but it might not necessarily tell us where to put the threshold. Still, it's useful enough to know that the way to make a good decision would be in terms of a particular threshold. Let me make this more specific. We can take our inspiration from the solution to the hypothesis testing problem that we had in the Bayesian case. In the Bayesian case, we just pick the hypothesis which is more likely, given the data. These posterior probabilities using Bayes' rule, they're written this way. And this term is the same as that term. They cancel out. Then let me collect terms here and there. I get an expression here. I think the version you have in your handout is the correct one. The one on the slide was not the correct one, so I'm fixing it here. This is the form of how you make decisions in the Bayesian case. What you do in the Bayesian case, you calculate this ratio. Let's call it the likelihood ratio. And compare that ratio to a threshold. And the threshold that you should be using in the Bayesian case has something to do with the prior probabilities of the two hypotheses. In the non-Bayesian case, we do not have prior probabilities, so we do not know how to set this threshold. But what we're going to do is we're going to keep this particular structure anyway and maybe use some other considerations to pick the threshold. So, we are going to use a likelihood ratio test, that's how it's called, in which we calculate a quantity of this kind that we call the likelihood and compare it with a threshold. So, what's the interpretation of this likelihood? We ask the x's that I have observed, how likely were they to occur if H1 was true? And how likely were they to occur if H0 was true? This ratio could be big if my data are plausible, they might occur under H1, but they're very implausible, extremely unlikely to occur under H0. Then my thinking would be, well, the data that I saw are extremely unlikely to have occurred under H0, so H0 is probably not true, I'm going to go for H1 and choose H1. So, when this ratio is big, it tells us that the data that we're seeing are better explained if we assume H1 to be true rather than H0 to be true. So, I calculate this quantity, compare it with a threshold, and that's how I make my decision. So, in this particular picture, for example, the way it would go would be the likelihood ratio in this picture goes monotonically with my x, so comparing the likelihood ratio to the threshold would be the same as comparing my x to the threshold, and we got the question of how to choose the threshold. The way that the threshold is chosen is usually done by fixing one of the two probabilities of error. That is, I say that I want my error of one particular type to be a given number, so I fix this alpha, and then I try to find where my threshold should be so that this tail probability out there is just equal to alpha. And then the other probability of error, beta, will be whatever it turns out to be. So, somebody picks alpha ahead of time, the probability of a false rejection. Based on alpha, I find where my threshold is going to be. I choose my threshold, and that determines subsequently the value of beta. So, we're going to continue with this story next time. And we'll stop here. Thank you.